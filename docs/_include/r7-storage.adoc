:sectnums:
:sectnumlevels: 3
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

:toc:
:toclevels: 1

= Storage Management: lvm, ssm, vdo

With the introduction of RHEL 7, two new storage management interfaces are available:

  * System Storage Manager (SSM)
  * libStorageMgmt

System Storage Manager (ssm) is a new command line interface for consolidated storage operations which integrates logical volume management, device mapper (DM), multiple-device (MD) and file-system operations into one unified user interface.

The libStorageMgmt package is a storage array independent Application Programming Interface (API).  It provides a stable and consistent API that allows developers the ability to programmatically manage different storage arrays and leverage the hardware accelerated features provided.  This library is used as a building block for other higher level management tools and applications.  End system administrators can also use it as a tool to manually manage storage and automate storage management tasks with the use of scripts. 		

With the release of RHEL 7.5, Red Hat added new technology into storage stack allowing for compression and deduplication.  The technology came by way of the acquisition of a company called Permabit and it is called the Virtual Data Optimizer... or VDO.

== Getting Started

Starting on the host *workstation.example.com*, let's ssh over to *node1.example.com*.  No password should be required.

.[root@workstation]#
----
ssh node1.example.com
----

Verify that you are on the right host for these exercises.

.[root@node1]#
----
cheat-storage-checkhost.sh
----

Now you are ready to begin

== Logical Volume Manager (LVM) Review

Let us review the traditional steps administrators perform to manually to construct a working file-system from the ground up:

  * Partition a block device (fdisk or parted)
  * Create a physical volume (pvcreate)
  * Create a logical volume group (vgcreate)
  * Create a logical volume (lvcreate)
  * Create a filesystem (mkfs)
  * Mount the filesystem (mount)
  * Create entry in /etc/fstab for persistence

I have a separate workshop with focus on LVM and Storage management.  Some of that content will eventually be folded into this unit.

== System Storage Manager (SSM)

With SSM, the workflow to create filesystems is similplied:

  * Partition a block device (fdisk or parted)
  * Use a single ssm command to create the pv, vg, lv, filesystem and mount
  * Add entry in /etc/fstab for persistence

First, we need to install system-storage-manager utility.

.[root@workstation]#
----
yum install -y system-storage-manager
----

Notice that there are NO physical storage devices. We are using files mapped as block devices via device-mapper.
Create a Simple Filesystem

.[root@workstation]#
----
ssm list devices
----

.Command Output
[source,indent=4]
----
------------------------------------------------------------------
Device         Free     Used      Total  Pool          Mount point
------------------------------------------------------------------
/dev/fd0                        4.00 KB                           
/dev/vda                       10.00 GB                PARTITIONED
/dev/vda1                     500.00 MB                /boot      
/dev/vda2   0.00 KB  9.51 GB    9.51 GB  rhel_pwob-r7             
/dev/vdb                       20.00 GB                           
/dev/vdb1                       1.00 KB                           
/dev/vdb10                      1.00 GB                           
/dev/vdb11                     10.00 GB                           
/dev/vdb5                       1.00 GB                           
/dev/vdb6                       1.00 GB                           
/dev/vdb7                       1.00 GB                           
/dev/vdb8                       1.00 GB                           
/dev/vdb9                       1.00 GB                           
------------------------------------------------------------------
----

WARNING: If you don't see `/dev/vdb5-vdb11`, then stop and rerun the ansible playbook as described at the top of this lab.

NOTE: it is possible your workshop environment was configured with different hardware and the partitions are reported as `/dev/sdb{5-11}` and not `/dev/vdb{5-11}`.  Just make a mental note and use the correct device paths for the exercises below.

.[root@workstation]#
----
ssm list volumes
----

.Command Output
[source,indent=4]
----
-------------------------------------------------------------------------------------------
Volume                  Pool          Volume size  FS     FS size       Free  Type    Mount point
-------------------------------------------------------------------------------------------
/dev/rhel_pwob-r7/swap  rhel_pwob-r7      1.00 GB                             linear
/dev/rhel_pwob-r7/root  rhel_pwob-r7      8.51 GB  xfs    8.50 GB    2.72 GB  linear  /
/dev/vda1                               500.00 MB  xfs  496.67 MB  284.79 MB  part    /boot
-------------------------------------------------------------------------------------------
----

=== Traditional Filesystem

==== Creation

.[root@workstation]#
----
ssm -f create --fstype ext4 /dev/vdb5 /mnt/exercise1
----

.Command Output
[source,indent=4]
----
  Physical volume "/dev/vdb5" successfully created.
  Volume group "lvm_pool" successfully created
  Logical volume "lvol001" created.
mke2fs 1.42.9 (28-Dec-2013)
Filesystem label=
OS type: Linux
Block size=4096 (log=2)
Fragment size=4096 (log=2)
Stride=0 blocks, Stripe width=0 blocks
65280 inodes, 261120 blocks
13056 blocks (5.00%) reserved for the super user
First data block=0
Maximum filesystem blocks=268435456
8 block groups
32768 blocks per group, 32768 fragments per group
8160 inodes per group
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (4096 blocks): done
Writing superblocks and filesystem accounting information: done

Directory '/mnt/exercise1' does not exist! Create (Y/n/q) ? Y
----

.[root@workstation]#
----
df /mnt/exercise1
----

.Command Output
[source,indent=4]
----
Filesystem                   1K-blocks  Used Available Use% Mounted on
/dev/mapper/lvm_pool-lvol001   1011672  2564    940500   1% /mnt/exercise1
----


==== Removal and Cleanup

.[root@workstation]#
----
ssm -f remove /mnt/exercise1 lvm_pool
----

.Command Output
[source,indent=4]
----
Device '/dev/lvm_pool/lvol001' is mounted on '/mnt/exercise1' Unmount (N/y/q) ? Y
  Logical volume "lvol001" successfully removed
  Volume group "lvm_pool" successfully removed
----

The filesystem is unmounted, logical volume and group areremoved.  Last step is to wipe the physical volume metadata from the block device.

.[root@workstation]#
----
pvremove /dev/vdb5
----

.Command Output
[source,indent=4]
----
Labels on physical volume "/dev/vdb5" successfully wiped
----

=== RAID filesystems

==== Creating RAID-1 (Mirrored) Filesystem

.[root@workstation]#
----
ssm -f create \			
	--fstype ext4 \			
	--size 500M \			
	-p summitvg \			
	-n exercise2 \			
	-r 1 \				
	/dev/vdb{6,7} /mnt/exercise2
----

.Command Output
[source,indent=4]
----
Physical volume "/dev/mapper/loop1p1" successfully created	
Physical volume "/dev/mapper/loop2p1" successfully created	
Volume group "summitvg" successfully created			
Rounding up size to full physical extent 52.00 MiB		
Logical volume "exercise2" created.				
meta-data=/dev/summitvg/exercise2 isize=256	agcount=2, agsize=6656 blks
	=		sectsz=512	attr=2, projid32bit=1
	=		crc=0		finobt=0		
data	=		bsize=4096	blocks=13312, imaxpct=25
	=		sunit=0	swidth=0 blks	
naming	=version 2		bsize=4096	ascii-ci=0 ftype=0	
log	=internal log	bsize=4096	blocks=853, version=2
	=		sectsz=512	sunit=0 blks, lazy-count=1
realtime =none		extsz=4096	blocks=0, rtextents=0
----

.[root@workstation]#
----
df -Th /mnt/exercise2
----

.Command Output
[source,indent=4]
----
Filesystem                     Type  Size  Used Avail Use% Mounted on
/dev/mapper/summitvg-exercise2 ext4  477M  2.3M  445M   1% /mnt/exercise2
----

The additional parameter "-o" allows us to specify fields to add or remove to the output.  By using "+" or "-" in front of individual fields, you can add or remove those fields to the standard output.  In the following case, we are removing the "mirror log" and "pv move" columns to accommodate an output that fits this documents width.

.[root@workstation]#
----
lvs -o-mirror_log,move_pv summitvg				
----

.Command Output
[source,indent=4]
----
LV        VG       Attr       LSize   Pool Origin Data%  Meta%  Cpy%Sync Convert
exercise2 summitvg rwi-aor--- 500.00m                           100.00
----

=== Creating RAID-10 (Mirrored and Striped) Filesystem

.[root@workstation]#
----
ssm list volumes
----

.Command Output
[source,indent=4]
----
------------------------------------------------------------------------------------
Volume                   Volume size  FS      FS size       Free  Type    Mount point   
------------------------------------------------------------------------------------
/dev/rhel_pwob-r7/swap   1.00 GB                              linear                
/dev/rhel_pwob-r7/root   8.51 GB  xfs     8.50 GB    2.63 GB  linear  /             
/dev/summitvg/exercise2  500.00 MB  ext4  500.00 MB  448.94 MB  raid1   /mnt/exercise2
/dev/vda1                500.00 MB  xfs   496.67 MB  284.79 MB  part    /boot         
------------------------------------------------------------------------------------
----

.[root@workstation]#
----
ssm -f create \			
	--size 500M \			
	--fstype xfs \			
	--pool summitvg \		
	--name exercise3 \		
	--raid 10 \
/dev/vdb{7..10} /mnt/exercise3
----

.Command Output
[source,indent=4]
----
Physical	volume "/dev/mapper/loop3p1" successfully created	
Physical	volume "/dev/mapper/loop4p1" successfully created	
Physical	volume "/dev/mapper/loop5p1" successfully created	
Physical	volume "/dev/mapper/loop6p1" successfully created	
Volume group "summitvg" successfully extended		
Rounding	size (25 extents) up to stripe boundary size (26 extents).
Logical volume "exercise3" created.				
meta-data=/dev/summitvg/exercise3 isize=256	agcount=4, agsize=6640 blks
	=			sectsz=512	attr=2, projid32bit=1
	=			crc=0		finobt=0		
data	=			bsize=4096	blocks=26560, imaxpct=25
	=			sunit=16	swidth=64 blks	
naming	=version 2		bsize=4096	ascii-ci=0 ftype=0	
log	=internal log	bsize=4096	blocks=768, version=2
	=			sectsz=512	sunit=16 blks, lazy-count=1
realtime =none		extsz=4096	blocks=0, rtextents=0
----

.[root@workstation]#
----
df /mnt/exercise3			
----

.Command Output
[source,indent=4]
----
Filesystem			Type 1K-blocks	Used Available Use% Mounted on
/dev/mapper/summitvg-exercise3 xfs	103168	5472	97696	6% /mnt/exercise3
----

.[root@workstation]#
----
lvs summitvg
----

.Command Output
[source,indent=4]
----
LV		VG	Attr	LSize	Pool Origin Data%  Meta%	Move Log Cpy%Sync Convert
exercise2 summitvg rwi-aor---	52.00m		100.00
exercise3 summitvg rwi-aor--- 104.00m		100.00
----

.[root@workstation]#
----
lvs -o +segtype summitvg
----

.Command Output
[source,indent=4]
----
LV	VG	Attr	LSize	Pool Origin Data%	Meta%  Move Log Cpy%Sync Convert Type
exercise2 summitvg rwi-aor---	52.00m	100.00	raid1
exercise3 summitvg rwi-aor--- 104.00m	100.00	raid10
----


== Virtual Data Optimizer (VDO)

First, we need to ensure the vdo and kmod-kvdo packages are installed.

.[root@workstation]#
----
yum install kmod-kvdo vdo
----

Notice the the packages were already installed and you are ready to create your optimized filesystems.

VDO has not been integrated into SSM, and so the workflow will resemble the traditional manual approach with one extra step:

  * Partition a block device (fdisk or parted)
  * Configure VDO
  * Create a filesystem (mkfs)
  * Mount the filesystem (mount)
  * Create entry in /etc/fstab for persistence
  * Create an Optimized Filesystem

We will be leveraging a partition on `/dev/vdb`.  The disk has already be partitioned, so we will begin by configuring VDO.

.[root@workstation]#
----
vdo create 	--name=exercise4 --device=/dev/vdb11 --vdoLogicalSize=30G
mkfs.xfs -K /dev/mapper/exercise4
mkdir /mnt/exercise4
mount /dev/mapper/exercise4 /mnt/exercise4
----

To make the mount persistent across reboots, you need to either add a systemd unit to mount the filesystem, or add an entry to /etc/fstab. as follows:

./etc/fstab
----
## Add the following to /etc/fstab
/dev/mapper/exercise4 /mnt/exercise4 xfs defaults,x-systemd.requires=vdo.service 0 0
----


.[root@workstation]#
----
vdostats --human-readable

vdostats --verbose

df -h /mnt/exercise4
----

Let us now populate the filesystem with some content.  Create a bunch of random subdirectories in our new filesystems with the following command.

.[root@workstation]#
----
for i in {1..100} ; do mktemp -d /mnt/exercise4/XXXXXX ; done
----

Now we will copy the same content into each of the folders as follows.

NOTE: This could take a few minutes.

.[root@workstation]#
----
for i in /mnt/exercise4/* ; do echo "${i}" ; cp -rf /usr/share/locale $i ; done
----

The prevoius command should have copied approximately 100MB in 100 folders yielding about 10G of traditional fielsystem consumption.

Let us now check some statistics.  

.[root@workstation]#
----
du -sh /mnt/exercise4

df /mnt/exercise4

vdostats --human-readable
----

So in summary, we built a 30GB filesystem that only has 10GB of actual physical disk capacity.  We then copied 10GB of data into the filesystem, but after deduplication `vdostats --human-readbale` should reflect something near 4GB of available plysical space.

A few additional high-level things to know about VDO.  

First, the VDO systemd unit is installed and enabled by default when the vdo package is installed. This unit automatically runs the vdo start --all command at system startup to bring up all activated VDO volumes

Second, VDO uses a high-performance deduplication index called UDS to detect duplicate blocks of data as they are being stored. The deduplication window is the number of previously written blocks which the index remembers. The size of the deduplication window is configurable.  The index will require a specific amount of RAM and a specific amount of disk space.

Last, Red Hat generally recommends using a "sparse" UDS index for all production use cases. This indexing data structure requires approximately one-tenth of a byte of DRAM (memory) per block in its deduplication window. On disk, it requires approximately 72 bytes of disk space per block.

The default configuration of the index is to use a "dense" index. This index is considerably less efficient (by a factor of 10) in DRAM, but it has much lower (also by a factor of 10) minimum required disk space, making it more convenient for evaluation in constrained environments.

Please refer to the Red Hat Storage Administration Guide further information on provisioning and managing your data with VDO:

Red Hat Enterprise Linux Storage Administration Guide (VDO)

== Additional Resources

Red Hat Documentation

    * link:https://https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8-beta/html/installing_identity_management_and_access_control/deploying-session-recording[Deplying Session Recording on Red Hat Enterprise Linux]

[discrete]
== End of Unit

link:../RHEL7-Workshop.adoc#toc[Return to TOC]

////
Always end files with a blank line to avoid include problems.
////
