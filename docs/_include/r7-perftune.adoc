:sectnums:
:sectnumlevels: 3
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

= Performance Analysis and Tuning

There are many tools provided in RHEL 7 to help optimize and monitor performance. In this section, we are going to take a look at:

  * Performance Co-Pilot (PCP), 
  * tuned
  * Numad

While this lab focuses on RHEL 7, these tools are also available in more recent minor releases of RHEL 6 and can have a huge impact on system performance.

== Performance Co-Pilot

Performance Co-Pilot (PCP) is a suite of tools, services and libraries for acquiring, storing and analyzing system-level performance measurements.  PCP’s light-weight, distributed architecture makes it particularly well suited to centralized analysis of complex systems.
 
PCP is broken down into two primary components:
 
  * *PCP Collectors*: These are the parts of PCP that collect and extract performance data from various sources, e.g. the operating system kernel.
  * *PCP Monitors*:  These are the parts of PCP that display data collected from hosts (or archives) that have the PCP Collector installed. Many monitor tools are available as part of the core PCP release, while other (typically graphical) monitoring tools are available separately in the PCP GUI package.


=== Getting Started

Log into the workstation as root, install the necessary packages and start the services

----
yum install -y pcp pcp-gui
systemctl enable pmcd pmlogger
systemctl start pmcd pmlogger
systemctl status pmcd pmlogger
----

The default logging interval used by the pmlogger service is 60 seconds.  For the purposes of this lab, we will modify the configuration to capture data at 10 seconds.  Edit the configuration as described here:
 
----
vim /etc/pcp/pmlogger/control
----

----
### duplicate or modify the following line to match

#LOCALHOSTNAME  n  n  PCP_LOG_DIR/pmlogger/mysummary  -r -T24h10m -c config.Summary

workstation  y  n  PCP_LOG_DIR/pmlogger/workstation  -r -T24h10m -c config.default -t 10s
----

Having made this configuration change, you will need to restart the pmlogger service

----
systemctl restart pmlogger
----


=== Memory Utilization: pmval

Next, we are going to use the `pmval` command to look at memory utilization for a specific time range:   <hr>:<min>:00 to <hr>:<min>:59 iin the latest performance archive log.
 
`pmval`  prints current or archived value for the nominated performance metric.
 
Note: Additional examples and command options are given by running man pmval on the command line 

----
cd /var/log/pcp/pmlogger/workstation
 
ls -lart
----
 
Use the most recent log file (ends in .0) at the bottom of the list for the next command

----
pmval -a 20160606.00.32.0 mem.util.used
----

Based on the output, select a relevant start and end time range to analyze 1min of performance data.
 
----
pmval -a 20160606.00.32.0 -S @00:33:00 -T @00:33:55 mem.util.used
----

Per the pmval output, we were able to go back in time and look at memory utilization history on workstation during a recent 60 second interval

[discrete]
==== Additional Metrics

There is an extensive list of pmval metrics like mem.util.used. To list all available metrics and their descriptions, run the following command:

----
pminfo -t 
pminfo -t | grep mem.util.used
----

NOTE: mem.util.used = used memory metric from /proc/meminfo
 
Explore the metric list and see what else interests you and test it out with pmval
pmstat - Overall System Performance

We can also review the overall system performance using pmstat command. The following command used logfile 20160606.00.32.0 to display 7 statistic entries starting from 00:33:00.

To test the same command, please change the date-stamped filename and start time accordingly

----
pmstat -a 20160606.00.32.0 -S @00:33:00 -s 7
----

----
@ Sat Jun 6 00:33:00 2016
 
Loadavg        			memory     swap      io     system    cpu
1 min   swpd    free   buff  cache   pi  po  bi  bo  in cs  us  sy  id


 0.04    0     1556m   764 145864    0   0   0   8   34 61  0   0   100
 0.04    0     1556m   764 145864    0   0   0  18   35 60  0   0   100
 0.03    0     1556m   764 145868    0   0   0  10   34 59  0   0   100
 0.03    0     1556m   764 145868    0   0   0   0   33 57  0   0   100
 0.03    0     1556m   764 145876    0   0   0   3   33 58  0   0   100
 0.03    0     1556m   764 145876    0   0   0   6   33 59  0   0   100
 0.02    0     1556m   764 145884    0   0   0   8   36 63  0   0   100
----

=== Additional PCP Monitors

PCP includes other monitors to view and analyze collector data:
 
  * pmstat: Displays data similar to vmstat
  * pmatop: Provides a top-like view
  * pmchart: Graphical display  ⇐ this is missing
  * pmie: Automate actions based on performance metrics
 
NOTE: Remember that these tools can work across remote systems to analyze more complicated systems (think 3-tier applications)

==== GUI Charting

NOTE: If you are using the graphical console of the workstation VM, you should be able to run pmchart from the command line.

----
pmchart
----

  * Click the second icon from the left to “Open View”
  * Look through the available views and select Overview
  * Next, click File → New Chart and view how granular the available metrics are
  * Select cgroups  → cpuacct → usage        	(cgroup.groups.cpuacct.usage)
  * Explore other metrics and chart their live performance
 
This tool can be used to “playback” collector data to help find root cause analysis.


== Standard Tuning Profiles: tuned

`tuned` is a tuning daemon that can adapt the operating system for better performance.  Red Hat provides tuning profiles to enhance the most commonly used workloads. In RHEL 7, `tuned` is enabled by default and also makes an intelligent decision about which profile to run out of the box. The concept of configuration inheritance has also been added in this release to make the profiles easier to customize.
 
For a full list of current tuning profiles:

----
man tuned-profiles
----

=== Getting Started
 
Log into the workstation as root and identify the running profile.
 
----
tuned-adm active
----

----
Current active profile: virtual-guest
----

tuned-adm can assess your system and make a tuning profile recommendation. This also sets the default profile for your system at install time
 
----
tuned-adm recommend 
----

----
virtual-guest
----

Next, list the available profiles on your system
 
----
tuned-adm list
----

----
Available profiles: 
- balanced 
-  desktop
..<SNIP>..
-  virtual-host

Current active profile: virtual-guest
----

TIP: The details of the profiles can be found in the man page `man tuned-profiles`

=== Change the Current Tuning Profile

To switch to another existing tuned profile (ex: powersave), use the tuned-adm command.
 
# tuned-adm profile powersave

Now use tuned-adm again to verify that your system tuning profile is now set to powersave.


# tuned-adm active

Current active profile: powersave
Customizing a Tuning Profile
Let us say our system is running an application that works well with the virtual-guest profile but not with Transparent Hugepages (THP). Examples of workloads where THP are not optimal include: SAP HANA, DB2, Datastage, Ambari, etc.

Note: Red Hat includes specific bare metal and virt profiles for SAP HANA with RHEL.
 
Begin by checking the current status of THP


# cat /sys/kernel/mm/transparent_hugepage/enabled
[always] madvise never

Now let us create a directory for our custom configuration and then create a config which inherits virtual-guest and then modifies THP.

# cd /usr/lib/tuned

# mkdir virtual-guest-no-thp
# cd virtual-guest-no-thp 

# vim tuned.conf
 
### add the following contents
 
[main] 
include=virtual-guest
 
[vm] 
transparent_hugepages=never

Now save the file and load the new tuning profile.  Finally, check your work.

# tuned-adm profile virtual-guest-no-thp
# tuned-adm active

Current active profile: virtual-guest-no-thp
 
# cat /sys/kernel/mm/transparent_hugepage/enabled

always madvise [never]
 
By looking at other tuned.conf files in /usr/lib/tuned, you will notice that other profile's tuned.conf contains a [sysctl] section. It is common practice to place sysctl tunings in /etc/sysctl.conf so they are set on boot, however tuned provides a mechanism for maintaining these types of tunables as well as others like disk scheduling and power settings for workload profiles.
Disabling Tuned
tuned is simple to disable if you choose not to run it.
 
# tuned-adm off 
# tuned-adm active 
No current active profile.
 
# systemctl stop tuned.service
# systemctl disable tuned.service
Optimizing NUMA
 
Historically, NUMA has been one of the most important items to tune and account for on larger systems. The RHEL 7 kernel implements automatic NUMA balancing for hardware with NUMA properties. Both following conditions are required:
 
numactl: hardware shows multiple nodes
NUMA flags: NUMA options can be set in /sys/kernel/debug/sched_features
 
This is the first release of RHEL where, out of the box, NUMA will require little to no tuning considerations for most workloads. The kernel is NUMA aware and in most cases will simply “do the right thing”. That said, there are still edge cases where certain workloads will perform better with manual pinning (numctl) or from running numad.
 
Getting Started
 
Red Hat introduced numad (an automatic NUMA affinity management daemon) in RHEL 6.3. It is a userspace tool that aims to improve out-of-the-box NUMA system performance for any long running, significant resource consuming processes (ex: KVM processes, HPC applications, etc…). It is not likely to help with processes that run only a few minutes, don't do much processing or don’t use much memory.
 
By default, numad is not installed on a RHEL 7 host. The following steps will walk you through installing and enabling numad on Red Hat Enterprise Linux 7.

# yum install numad numactl

# systemctl enable numad.service
# systemctl start numad.service
# systemctl status numad.service



numactl lets administrators run a process with a specified scheduling or memory placement policy.  It can also set a persistent policy for shared memory segments or files, and set the processor affinity and memory affinity of a process.  Granted this is not too exciting on our small lab VM, but let's look at the current resources on your VM using numactl:

# numactl --hardware 
available: 1 nodes (0)
node 0 cpus: 0 1
node 0 size: 4095 MB
node 0 free: 2294 MB
node distances:
node   0
  0:  10

Now run lscpu to gather CPU architecture information from sysfs and /proc/cpuinfo 

# lscpu
Architecture:      	x86_64
CPU op-mode(s):    	32-bit, 64-bit
Byte Order:        	Little Endian
CPU(s):            	2
On-line CPU(s) list:   0,1
Thread(s) per core:	1
Core(s) per socket:	1
Socket(s):         	2
NUMA node(s):      	1
Vendor ID:         	GenuineIntel
CPU family:        	6
Model:             	15
Model name:        	Intel(R) Xeon(R) CPU @ 2.50GHz
Stepping:          	11
CPU MHz:           	2499.998
BogoMIPS:          	4999.99
Hypervisor vendor: 	KVM
Virtualization type:   full
L1d cache:         	32K
L1i cache:         	32K
L2 cache:          	4096K
NUMA node0 CPU(s): 	0,1

Let's interpret the output from the previous commands.  Based on the lscpu output, it shows that this VM has 1 NUMA node, 2 CPU sockets, and 2 CPU cores. numactl is also reporting that our single NUMA node host has total of 4095 MB of memory with 2294 MB free currently.

Note: Your output may differ due the the lab environment


Sample numactl Output From A Larger Host
In a multi-CPU server environment, numactl is able to display additional information about the CPU placements on the motherboard. Here is the numactl output of a multi-CPU server: 


Note: this output comes from a different physical host and provided as an example.

# numactl --hardware 
available: 4 nodes (0-3) 
node 0 cpus: 0 4 8 12 16 20 24 28 32 36 node 0 size: 65415 MB 
node 0 free: 43971 MB 

node 1 cpus: 2 6 10 14 18 22 26 30 34 38 node 1 size: 65536 MB 
node 1 free: 44321 MB 

node 2 cpus: 1 5 9 13 17 21 25 29 33 37 node 2 size: 65536 MB 
node 2 free: 44304 MB 

node 3 cpus: 3 7 11 15 19 23 27 31 35 39 node 3 size: 65536 MB 
node	3 free: 44329	MB

node	distances:	
node	0	1	2	3

0:  10  21  21  21
1:  21  10  21  21 
2:  21  21  10  21
3:  21  21  21  10





Sample lscpu Output From A Larger Host
Here is lscpu output of a multi-CPU server

Note: this output comes from a different physical host and provided as an example.

# lscpu 
Architecture:	x86_64
CPU op-mode(s):	32-bit, 64-bit
Byte Order:	Little Endian
CPU(s):	40
On-line CPU(s) list:	0-39
Thread(s) per core:	1
Core(s) per socket:	10
Socket(s):	4
NUMA node(s):	4
Vendor ID:	GenuineIntel
CPU family:	6
Model:	47
Model name:	Intel(R) Xeon(R) CPU E7- 4870  @ 2.40GHz
Stepping:	2
CPU MHz:	2394.204
BogoMIPS:	4787.85
Virtualization:	VT-x
L1d cache:	32K
L1i cache:	32K
L2 cache:	256K
L3 cache:	30720K
NUMA node0 CPU(s):	0,4,8,12,16,20,24,28,32,36
NUMA node1 CPU(s):	2,6,10,14,18,22,26,30,34,38
NUMA node2 CPU(s):	1,5,9,13,17,21,25,29,33,37
NUMA node3 CPU(s):	3,7,11,15,19,23,27,31,35,39

Based on previous outputs of our larger host, numactl is able to display current free and total memory that is local to each NUMA node. Also the relative distance between 2 CPU sockets on the motherboard. Based on node distance information from 'numactl --hardware', we know that any given CPU has direct connection to another CPU. CPU 0's distance to CPU 0 is 10 (the shortest), to CPU 1 is 21, to CPU 2 is 21 and to CPU 3 is 21. (i.e. same distance from CPU 0 to CPU 1, 2, and 3) 

NUMA Statistics
The numastat tool displays per-NUMA node memory statistics for processes and the operating system.  It shows administrators whether process memory is spread throughout a system or centralized on specific nodes.

# numastat -v	
Per-node numastat info (in Mbs):		
                 Node 0          Total
	           --------------- ---------------
Numa_Hit        11718.43         11718.43
Numa_Miss       0.00             0.00
Numa_Foreign    0.00             0.00
Interleave_Hit  46.96            46.96
Local_Node      11718.43         11718.43
Other_Node      0.00             0.00

To find a description of each value displayed above or other numastat options, review the man page for numastat. 

# man numastat 

Most importantly to look out for are: numa_miss, numa_foreign and other_node values. A high value indicates a process has attempted to get a page from its local NUMA node, but it was out of free pages and the system had to allocate free pages from another NUMA node.


Below is an example of a RHEL 6 hypervisor running without numad. Notice the VMs are split almost evenly across the sockets.




Next is the same hardware running numad. Notice the NUMA alignment is almost perfect and the Numa_Miss count dropped from ~2300 to ~7.




Enable and Disable NUMA Balancing
Disable To disable/enable system-wide automatic NUMA balancing, use the following commands

To disable NUMA balancing:
# echo 0 > /proc/sys/kernel/numa_balancing


To enable NUMA balancing:
# echo 1 > /proc/sys/kernel/numa_balancing



== Additional Resources

Red Hat Documentation

    * link:https://https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8-beta/html/installing_identity_management_and_access_control/deploying-session-recording[Deplying Session Recording on Red Hat Enterprise Linux]

[discrete]
== End of Unit

link:../RHEL7-Workshop.adoc#toc[Return to TOC]

////
Always end files with a blank line to avoid include problems.
////
